{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa9f358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a563e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = data.text\n",
    "        self.ctext = data.ctext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec50fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    inputs_text = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            input_text = [tokenizer.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for id in ids]\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "            \n",
    "            inputs_text.extend(input_text)\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return inputs_text, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "30b317e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"MAX_LEN\" : 512,\n",
    "          \"TRAIN_BATCH_SIZE\" : 2,\n",
    "          \"VALID_BATCH_SIZE\" : 2,\n",
    "          \"TRAIN_EPOCHS\" : 2,\n",
    "          \"VAL_EPOCHS\" : 1,\n",
    "          \"LEARNING_RATE\" : 1e-4,\n",
    "          \"SEED\" : 42,\n",
    "          \"MAX_LEN\" : 512,\n",
    "          \"SUMMARY_LEN\" :150        \n",
    "         }\n",
    "  \n",
    "config[\"MAX_LEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5febf506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               ctext  \\\n",
      "0  summarize: By . Associated Press . PUBLISHED: ...   \n",
      "1  summarize: (CNN) -- Ralph Mata was an internal...   \n",
      "2  summarize: A drunk driver who killed a young w...   \n",
      "3  summarize: (CNN) -- With a breezy sweep of his...   \n",
      "4  summarize: Fleetwood are the only team still t...   \n",
      "\n",
      "                                                text  \n",
      "0  Bishop John Folda, of North Dakota, is taking ...  \n",
      "1  Criminal complaint: Cop used his role to help ...  \n",
      "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
      "3  Nina dos Santos says Europe must be ready to a...  \n",
      "4  Fleetwood top of League One after 2-0 win at S...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./cnn_dailymail/train.csv',encoding='latin-1') \n",
    "df = df[['article','highlights']]\n",
    "df = df.rename({\"article\" : \"ctext\", \"highlights\" : \"text\"}, axis = 1)\n",
    "df.ctext = 'summarize: ' + df.ctext\n",
    "df = df[:10]\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f376772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/opt/miniconda3/envs/kaggle_env/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ab3dfa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state = 42)\n",
    "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce707e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (10, 2)\n",
      "TRAIN Dataset: (8, 2)\n",
      "TEST Dataset: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(val_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47d82c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomDataset(data = train_dataset, tokenizer = tokenizer,source_len = config[\"MAX_LEN\"], \n",
    "                             summ_len = config[\"SUMMARY_LEN\"])\n",
    "val_set = CustomDataset(data = val_dataset, tokenizer = tokenizer,source_len = config[\"MAX_LEN\"], \n",
    "                             summ_len = config[\"SUMMARY_LEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f1153f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "        'batch_size': config[\"TRAIN_BATCH_SIZE\"],\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "}\n",
    "\n",
    "val_params = {\n",
    "        'batch_size': config[\"VALID_BATCH_SIZE\"],\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "50f31378",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d07653db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b1603b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=config[\"LEARNING_RATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a466260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/hugo/opt/miniconda3/envs/kaggle_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  7.836664199829102\n",
      "Epoch: 0, Loss:  4.574958324432373\n",
      "Epoch: 0, Loss:  4.176817417144775\n",
      "Epoch: 0, Loss:  4.8595123291015625\n",
      "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
      "Completed 0\n",
      "Output Files generated for review\n",
      "Epoch: 1, Loss:  3.9061052799224854\n",
      "Epoch: 1, Loss:  2.2678771018981934\n",
      "Epoch: 1, Loss:  3.5002641677856445\n",
      "Epoch: 1, Loss:  2.4181504249572754\n",
      "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
      "Completed 0\n",
      "Output Files generated for review\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "for epoch in range(config[\"TRAIN_EPOCHS\"]):\n",
    "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "    # Saving the dataframe as predictions.csv\n",
    "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "    for epoch in range(config[\"VAL_EPOCHS\"]):\n",
    "        input_text, predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Input Text ': input_text, 'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv('./predictions.csv')\n",
    "        print('Output Files generated for review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7cb071f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"summarize: A NASA spacecraft will deliberately slam into an asteroid Monday, and it's all in the name of planetary protection. The DART mission, or the Double Asteroid Redirection Test, will crash into the space rock at 7:14 p.m. ET after launching 10 months ago.The spacecraft will attempt to affect the motion of an asteroid in space. A live stream of images captured by the spacecraft will be available on NASA's website beginning at 6 p.m. ET. The mission is heading for Dimorphos, a small moon orbiting the near-Earth asteroid Didymos. The asteroid system poses no threat to Earth, NASA officials have said, making it a perfect target to test out a kinetic impact -- which may be needed if an asteroid is ever on track to hit Earth. The event will be the agency's first full-scale demonstration of deflection technology that can protect the planet. 'For the first time ever, we will measurably change the orbit of a celestial body in the universe,' said Robert Braun, head of the Johns Hopkins University Applied Physics Laboratory's Space Exploration Sector in Laurel, Maryland.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a76f48d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[21603,    10,    71, 15971,   628,  6696,    56, 24067,     3,     7,\n",
       "            40,   265,   139,    46,     3,  8323,  8184,  2089,     6,    11,\n",
       "            34,    31,     7,    66,    16,     8,   564,    13,     3, 30351,\n",
       "          1711,     5,    37,   309,  8241,  2253,     6,    42,     8,  8405,\n",
       "            71,   849,  8184,  1624,  2060,  4985,  2300,     6,    56,  8420,\n",
       "           139,     8,   628,  2480,    44,   489,    10,  2534,     3,   102,\n",
       "             5,    51,     5, 10104,   227,     3, 14138,   335,   767,   977,\n",
       "             5,   634,   628,  6696,    56,  3332,    12,  2603,     8,  4644,\n",
       "            13,    46,     3,  8323,  8184,    16,   628,     5,    71,   619,\n",
       "          6093,    13,  1383,  9534,    57,     8,   628,  6696,    56,    36,\n",
       "           347,    30, 15971,    31,     7,   475,  1849,    44,   431,     3,\n",
       "           102,     5,    51,     5, 10104,     5,    37,  2253,    19,  6904,\n",
       "            21,  2043,  8886,    32,     7,     6,     3,     9,   422,  8114,\n",
       "         15607,    53,     8,  1084,    18,   427, 10702,     3,  8323,  8184,\n",
       "          3963,    63,  3972,     5,    37,     3,  8323,  8184,   358,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = tokenizer.batch_encode_plus([sentence], max_length= 150, pad_to_max_length=True,return_tensors='pt')\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a388f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "                input_ids = tokenized_sentence[\"input_ids\"],\n",
    "                attention_mask = tokenized_sentence[\"attention_mask\"], \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7b314349",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc56dc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the spacecraft will crash into the space rock at 7:14 p.m. ET after launching 10 months ago. the mission is heading for Dimorphos, a small moon orbiting the near-Earth asteroid Didymos.']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7fb87a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
